{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Cluster Initialization with MPI4py: This could only be run on a HPC cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The is only relevant to running mpi4py in a Jupyter notebook.\n",
    "import ipyparallel\n",
    "cluster=ipyparallel.Client(profile='mpi_tutorial')\n",
    "print(\"IDs:\",cluster.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px \n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "print (\"I'm rank %d of %d on %s\" %(rank,size,MPI.Get_processor_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import numpy as np\n",
    "from numpy import math\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specification: OU Process\n",
    " 1. $dX_{t} = \\theta_{1}(\\theta_{2} - X_{t})dt + \\sigma dW_{t}$, $Y_{t}|X_{t} \\sim \\mathcal{N}(X_{t}, \\theta_{3}^2)$\n",
    " 2. $\\mathbb{E}[X_{t}] = x_{0} e^{-\\theta_1t} + \\theta_{2} (1-e^{-\\theta_{1}t})$, $Var[X_{t}] = \\frac{\\sigma^{2}}{2\\theta_{1}}(1-e^{-2t\\theta_1})$\n",
    " 3. $Y_{1},Y_{2},...$ mutually independent, $Y_{t} \\sim_{i.i.d.} \\mathcal{N}(\\mathbb{E}[X_{t}], \\theta_{3}^2 + Var[X_{t}])$, for $t \\in \\mathbb{N}_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "initial_val = 1\n",
    "sigma = 0.5\n",
    "theta = np.array([1,0,np.sqrt(0.2)])\n",
    "\n",
    "\n",
    "def diff_coef(x, dt, dw):\n",
    "    return sigma*np.math.sqrt(dt)*dw\n",
    "\n",
    "def drift_coef(x, dt):\n",
    "    return theta[0]*(theta[1]-x)*dt\n",
    "\n",
    "# Log-scaled unnormalized likelihood function p(y|x)\n",
    "def likelihood_logscale(y, x):\n",
    "    d = (y-x)\n",
    "    gn = -1/2*(d**2/(theta[2]**2))\n",
    "    return gn\n",
    "\n",
    "def likelihood_update(y,un,unormal_weight):\n",
    "    gamma = math.sqrt(0.2)\n",
    "    d = (y-un)\n",
    "    gn1 = -1/2*(d**2/(theta[2]**2)) + unormal_weight\n",
    "    return gn1\n",
    "\n",
    "def sig_mean(t,theta):\n",
    "    return initial_val*np.exp(-theta[0]*t) + theta[1]*(1-np.exp(-theta[0]*t))\n",
    "\n",
    "## Used only when theta[0] != 0\n",
    "def sig_var(t,theta):\n",
    "    return (sigma**2 / (2*theta[0])) * (1-np.exp(-2*theta[0]*t))\n",
    "\n",
    "def gen_data(T):\n",
    "    Y = np.zeros(T+1)\n",
    "    for t in range(T+1):\n",
    "        std = np.sqrt(sig_var(t,theta) + theta[2]**2)\n",
    "        Y[t] = sig_mean(t,theta) + std * np.random.randn(1)\n",
    "    return Y\n",
    "\n",
    "def Kalmanfilter(T,Y):\n",
    "    \n",
    "    m = np.zeros((T+1))\n",
    "    mhat = np.zeros((T+1))\n",
    "    c = np.zeros((T+1))\n",
    "    a = theta[0]\n",
    "    s = sigma\n",
    "    # observational noise variance is gam^2*I\n",
    "    gam = theta[2]\n",
    "    # dynamics noise variance is sig^2*I\n",
    "    sig = np.sqrt(s**2/2/a*(1-np.exp(-2*a)))\n",
    "    # dynamics determined by A\n",
    "    A = np.exp(-a)\n",
    "    # initial mean&covariance\n",
    "    m[0] = initial_val\n",
    "    c[0] = 0\n",
    "    H = 1\n",
    "    # solution & assimilate!\n",
    "    for t in range(T):\n",
    "        mhat[t] = A*m[t] + theta[1]*(1-A)\n",
    "        chat = A*c[t]*A + sig**2\n",
    "        ########################\n",
    "        d = Y[t+1] - H*mhat[t]\n",
    "        # Kalmab Gain\n",
    "        K = (chat*H) / (H*chat*H + gam**2)\n",
    "        # Mean Update\n",
    "        m[t+1] = mhat[t] + K*d\n",
    "        # Covariance update\n",
    "        c[t+1] = (1-K*H)*chat\n",
    "    tv = m[T]\n",
    "    return tv\n",
    "\n",
    "def Kalmanfilter_path(T,Y):\n",
    "    \n",
    "    m = np.zeros((T+1))\n",
    "    mhat = np.zeros((T+1))\n",
    "    c = np.zeros((T+1))\n",
    "    a = theta[0]\n",
    "    s = sigma\n",
    "    # observational noise variance is gam^2*I\n",
    "    gam = theta[2]\n",
    "    # dynamics noise variance is sig^2*I\n",
    "    sig = np.sqrt(s**2/2/a*(1-np.exp(-2*a)))\n",
    "    # dynamics determined by A\n",
    "    A = np.exp(-a)\n",
    "    # initial mean&covariance\n",
    "    m[0] = initial_val\n",
    "    c[0] = 0\n",
    "    H = 1\n",
    "    # solution & assimilate!\n",
    "    for t in range(T):\n",
    "        mhat[t] = A*m[t] + theta[1]*(1-A)\n",
    "        chat = A*c[t]*A + sig**2\n",
    "        ########################\n",
    "        d = Y[t+1] - H*mhat[t]\n",
    "        # Kalmab Gain\n",
    "        K = (chat*H) / (H*chat*H + gam**2)\n",
    "        # Mean Update\n",
    "        m[t+1] = mhat[t] + K*d\n",
    "        # Covariance update\n",
    "        c[t+1] = (1-K*H)*chat\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Resampling - input one-dimensional particle x\n",
    "def resampling(weight, gn, x, N):\n",
    "    ess = 1/((weight**2).sum())\n",
    "    if ess <= (N/2):\n",
    "        ## Sample with uniform dice\n",
    "        dice = np.random.random_sample(N)\n",
    "        ## np.cumsum obtains CDF out of PMF\n",
    "        bins = np.cumsum(weight)\n",
    "        ## np.digitize gets the indice of the bins where the dice belongs to \n",
    "        x_hat = x[np.digitize(dice,bins)]\n",
    "        ## after resampling we reset the accumulating weight\n",
    "        gn = np.zeros(N)\n",
    "    if ess > (N/2):\n",
    "        x_hat = x\n",
    "    \n",
    "    return x_hat, gn\n",
    "\n",
    "# Coupled Wasserstein Resampling\n",
    "def coupled_wasserstein(fine_weight, coarse_weight, gn, gc, fine_par, coarse_par, N):\n",
    "    ess = 1/((fine_weight**2).sum())\n",
    "    fine_hat = fine_par\n",
    "    coarse_hat = coarse_par\n",
    "    if ess <= (N/2):\n",
    "        # Sort in ascending order of particles\n",
    "        ind = np.argsort(fine_par[:])\n",
    "        inc = np.argsort(coarse_par[:])\n",
    "        fine_par = fine_par[ind]\n",
    "        fine_weight = fine_weight[ind]\n",
    "        coarse_par = coarse_par[inc]\n",
    "        coarse_weight = coarse_weight[inc]\n",
    "        # Sample with uniform dice\n",
    "        dice = np.random.random_sample(N)\n",
    "        # CDF\n",
    "        bins = np.cumsum(fine_weight)\n",
    "        bins1 = np.cumsum(coarse_weight)\n",
    "        # get the indices of the bins where the dice belongs to\n",
    "        fine_hat = fine_par[np.digitize(dice, bins)]\n",
    "        coarse_hat = coarse_par[np.digitize(dice, bins1)]\n",
    "        # reset accumulating weight after resampling\n",
    "        gn = np.zeros(N)\n",
    "        gc = np.zeros(N)\n",
    "    if ess > (N/2):\n",
    "        fine_hat = fine_par\n",
    "        coarse_hat = coarse_par\n",
    "\n",
    "    return fine_hat, gn, coarse_hat, gc\n",
    "\n",
    "# Maixmally Coupled Resampling\n",
    "def coupled_maximal(fine_weight, coarse_weight, gn, gc, fine_par, coarse_par, N):\n",
    "    ess = 1/((fine_weight**2).sum())\n",
    "    if ess <= (N/2):\n",
    "        # Maximal coupled resampling\n",
    "        fine_hat, coarse_hat = maximal_resample(fine_weight, coarse_weight, fine_par, coarse_par, N)\n",
    "        # reset accumulating weight after resampling\n",
    "        gn = np.zeros(N)\n",
    "        gc = np.zeros(N)\n",
    "    if ess > (N/2):\n",
    "        fine_hat = fine_par\n",
    "        coarse_hat = coarse_par\n",
    "    return fine_hat, gn, coarse_hat, gc\n",
    "\n",
    "def maximal_resample(weight1,weight2,x1,x2,N):\n",
    "    # Initialize\n",
    "    x1_hat = np.zeros(N)\n",
    "    x2_hat = np.zeros(N)\n",
    "\n",
    "    # Calculating many weights\n",
    "    unormal_min_weight = np.minimum(weight1, weight2)\n",
    "    min_weight_sum = np.sum(unormal_min_weight)\n",
    "    min_weight = unormal_min_weight / min_weight_sum\n",
    "    unormal_reduce_weight1 = weight1 - unormal_min_weight\n",
    "    unormal_reduce_weight2 = weight2 - unormal_min_weight\n",
    "\n",
    "    ## Sample with uniform dice\n",
    "    dice = np.random.random_sample(N)\n",
    "    ## [0] takes out the numpy array which is suitable afterwards\n",
    "    coupled = np.where(dice <= min_weight_sum)[0]\n",
    "    independ = np.where(dice > min_weight_sum)[0]\n",
    "    ncoupled = np.sum(dice <= min_weight_sum)\n",
    "    nindepend = np.sum(dice > min_weight_sum)\n",
    "    \n",
    "    if ncoupled>=0:\n",
    "        dice1 = np.random.random_sample(ncoupled)\n",
    "        bins = np.cumsum(min_weight)\n",
    "        x1_hat[coupled] = x1[np.digitize(dice1,bins)]\n",
    "        x2_hat[coupled] = x2[np.digitize(dice1,bins)]\n",
    "   \n",
    "    ## nindepend>0 implies min_weight_sum>0 imples np.sum(unormal_reduce_weight*) is positive, thus the division won't report error\n",
    "    if nindepend>0:\n",
    "        reduce_weight1 = unormal_reduce_weight1 / np.sum(unormal_reduce_weight1)\n",
    "        reduce_weight2 = unormal_reduce_weight2 / np.sum(unormal_reduce_weight2)\n",
    "        dice2 = np.random.random_sample(nindepend)\n",
    "        bins1 = np.cumsum(reduce_weight1)\n",
    "        bins2 = np.cumsum(reduce_weight2)\n",
    "        x1_hat[independ] = x1[np.digitize(dice2,bins1)]\n",
    "        x2_hat[independ] = x2[np.digitize(dice2,bins2)]\n",
    "        \n",
    "    return x1_hat, x2_hat\n",
    "\n",
    "\n",
    "def Particle_filter(l,T,N,Y):\n",
    "    hl = 2**(-l)\n",
    "    un = np.zeros(N)+initial_val\n",
    "    un_hat = un\n",
    "    gn = np.zeros(N)\n",
    "    for t in range(T):\n",
    "        un_hat = un\n",
    "        for dt in range(2**l):\n",
    "            dw = np.random.randn(N)\n",
    "            un = un + drift_coef(un, hl) + diff_coef(un, hl, dw)\n",
    "        # Cumulating weight function    \n",
    "        gn = likelihood_logscale(Y[t+1], un) + gn\n",
    "        what = np.exp(gn-np.max(gn))\n",
    "        wn = what/np.sum(what)\n",
    "        \n",
    "        # Wasserstein resampling\n",
    "        un_hat, gn = resampling(wn, gn, un, N)\n",
    "    \n",
    "    return(np.sum(un*wn))\n",
    "\n",
    "\n",
    "def Coupled_particle_filter_wasserstein(l,T,N,Y):\n",
    "    hl = 2**(-l)\n",
    "    ## Initial value\n",
    "    un1 = np.zeros(N) + initial_val\n",
    "    cn1 = np.zeros(N) + initial_val\n",
    "    gn = np.ones(N)\n",
    "    gc = np.ones(N)\n",
    "    for t in range(T):\n",
    "        un = un1\n",
    "        cn = cn1\n",
    "        for dt in range(2**(l-1)):\n",
    "            dw = np.random.randn(2,N)\n",
    "            for s in range(2):\n",
    "                un = un + drift_coef(un, hl) + diff_coef(un, hl, dw[s,:])\n",
    "            cn = cn + drift_coef(cn, hl*2) + diff_coef(cn, hl, (dw[0,:] + dw[1,:]))\n",
    "        \n",
    "        ## Accumulating Weight Function\n",
    "        gn = likelihood_update(Y[t+1], un, gn)\n",
    "        what = np.exp(gn-np.max(gn))\n",
    "        wn = what/np.sum(what)\n",
    "        gc = likelihood_update(Y[t+1], cn, gc)\n",
    "        wchat = np.exp(gc-np.max(gc))\n",
    "        wc = wchat/np.sum(wchat)\n",
    "        ## Wassersteing Resampling \n",
    "        un1, gn, cn1, gc = coupled_wasserstein(wn,wc,gn,gc,un,cn,N)\n",
    "\n",
    "    return(np.sum(un*wn-cn*wc))\n",
    "\n",
    "\n",
    "def Coupled_particle_filter_maximal(l,T,N,Y):\n",
    "    hl = 2**(-l)\n",
    "    ## Initial value\n",
    "    un1 = np.zeros(N) + initial_val\n",
    "    cn1 = np.zeros(N) + initial_val\n",
    "    gn = np.ones(N)\n",
    "    gc = np.ones(N)\n",
    "    for t in range(T):\n",
    "        un = un1\n",
    "        cn = cn1\n",
    "        for dt in range(2**(l-1)):\n",
    "            dw = np.random.randn(2,N)\n",
    "            for s in range(2):\n",
    "                un = un + drift_coef(un, hl) + diff_coef(un, hl, dw[s,:])\n",
    "            cn = cn + drift_coef(cn, hl*2) + diff_coef(cn, hl, (dw[0,:] + dw[1,:]))\n",
    "        \n",
    "        ## Accumulating Weight Function\n",
    "        gn = likelihood_update(Y[t+1], un, gn)\n",
    "        what = np.exp(gn-np.max(gn))\n",
    "        wn = what/np.sum(what)\n",
    "        gc = likelihood_update(Y[t+1], cn, gc)\n",
    "        wchat = np.exp(gc-np.max(gc))\n",
    "        wc = wchat/np.sum(wchat)\n",
    "        ## Wassersteing Resampling \n",
    "        un1, gn, cn1, gc = coupled_maximal(wn,wc,gn,gc,un,cn,N)\n",
    "\n",
    "    return(np.sum(un*wn-cn*wc))\n",
    "\n",
    "def coef(x, y): \n",
    "    # number of observations/points \n",
    "    n = np.size(x) \n",
    "  \n",
    "    # mean of x and y vector \n",
    "    m_x, m_y = np.mean(x), np.mean(y) \n",
    "  \n",
    "    # calculating cross-deviation and deviation about x \n",
    "    SS_xy = np.sum(y*x) - n*m_y*m_x \n",
    "    SS_xx = np.sum(x*x) - n*m_x*m_x \n",
    "  \n",
    "    # calculating regression coefficients \n",
    "    b_1 = SS_xy / SS_xx \n",
    "    b_0 = m_y - b_1*m_x \n",
    "  \n",
    "    return(b_0, b_1) \n",
    "\n",
    "def num_coupled_par(p, p_max, const):\n",
    "    return int(2**(p+2*p_max) * (p_max**2) * const * c3)\n",
    "\n",
    "def num_par(p, p_max, const):\n",
    "    return int(2**(p+2*p_max) * (p_max**2) * const * c2)\n",
    "\n",
    "def prob_l_func(max_val):\n",
    "    prob = np.zeros(max_val)\n",
    "    for l in range(max_val):\n",
    "        prob[l] = 2**(-l*beta)\n",
    "    prob = prob / np.sum(prob)\n",
    "    return prob\n",
    "\n",
    "def prob_p_func(max_val):\n",
    "    prob = np.zeros(max_val)\n",
    "    for p in range(max_val):\n",
    "        prob[p] = 2**(-p)\n",
    "    prob = prob / np.sum(prob)\n",
    "    return prob\n",
    "\n",
    "def Xi_zero(T,p_prob,p_max,const,Y):\n",
    "    # sample the variable P\n",
    "    p = int(np.random.choice(p_max, 1, p=p_prob)[0])\n",
    "    #print('p_val is',p)\n",
    "    # construct the estimator\n",
    "    Xi_zero = (Particle_filter(0,T,num_par(p, p_max, const),Y) - Particle_filter(0,T,num_par(p-1, p_max, const),Y)) / p_prob[p]\n",
    "    return Xi_zero\n",
    "\n",
    "def Xi_nonzero(l,T,p_prob,p_max,const,Y):\n",
    "    # sample the variable P\n",
    "    p = int(np.random.choice(p_max, 1, p=p_prob)[0])\n",
    "    #print('p_val is',p)\n",
    "    # construct the estimator\n",
    "    Xi = (Coupled_particle_filter_maximal(l,T,num_coupled_par(p,p_max,const),Y) - Coupled_particle_filter_maximal(l,T,num_coupled_par(p-1,p_max,const),Y)) / p_prob[p]\n",
    "    return Xi\n",
    "\n",
    "def Xi(T,l_prob,l_max,p_prob,p_max,const,Y):\n",
    "    l = int(np.random.choice(l_max, 1, p=l_prob)[0])\n",
    "    #print('value of l is',l)\n",
    "    if l==0:\n",
    "        Xi = Xi_zero(T,p_prob,p_max,const,Y)\n",
    "    if l!=0:\n",
    "        Xi = Xi_nonzero(l,T,p_prob,p_max,const,Y)\n",
    "    est = Xi / l_prob[l]\n",
    "    return est\n",
    "    \n",
    "def parallel_particle_filter(M,T,max_val,const,Y):\n",
    "    l_max = max_val\n",
    "    p_max = max_val\n",
    "    l_prob = prob_l_func(l_max)\n",
    "    p_prob = prob_p_func(p_max)\n",
    "    est_summand = np.zeros(M)\n",
    "    for m in range(M):\n",
    "        est_summand[m] = Xi(T,l_prob,l_max,p_prob,p_max,const,Y)\n",
    "    return (np.mean(est_summand))\n",
    "\n",
    "def parallel_particle_filter_record_progbar(M,T,max_val,const,Y):\n",
    "    l_max = max_val\n",
    "    p_max = max_val\n",
    "    l_prob = prob_l_func(l_max)\n",
    "    p_prob = prob_p_func(p_max)\n",
    "    est_summand = np.zeros(M)\n",
    "    pr = progressbar.ProgressBar(max_value=M).start()\n",
    "    for m in range(M):\n",
    "        est_summand[m] = Xi(T,l_prob,l_max,p_prob,p_max,const,Y)\n",
    "        pr.update(m+1)\n",
    "    pr.finish()\n",
    "    return est_summand\n",
    "\n",
    "def parallel_particle_filter_record(M,T,max_val,const,Y):\n",
    "    l_max = max_val\n",
    "    p_max = max_val\n",
    "    l_prob = prob_l_func(l_max)\n",
    "    p_prob = prob_p_func(p_max)\n",
    "    est_summand = np.zeros(M)\n",
    "    for m in range(M):\n",
    "        est_summand[m] = Xi(T,l_prob,l_max,p_prob,p_max,const,Y)\n",
    "    return est_summand\n",
    "\n",
    "# For OU process, beta=2\n",
    "def num_ml_coupled(l,lmax,const):\n",
    "    return 2**(2*lmax-1.5*l) * const * c3\n",
    "\n",
    "def num_ml_single(l,lmax,const):\n",
    "    return 2**(2*lmax-1.5*l) * const * c2\n",
    "\n",
    "def mlpf(T,max_val,const,Y):\n",
    "    L = max_val\n",
    "    level_est = np.zeros(L)\n",
    "    level_est[0] = Particle_filter(0,T,int(num_ml_single(0,L,const)),Y)\n",
    "    for l in range(1,L):\n",
    "        level_est[l] = Coupled_particle_filter_maximal(l,T,int(num_ml_coupled(l,L,const)),Y)\n",
    "    return np.sum(level_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulation Setup Example\n",
    "1. At discretization level $l=2$, aim at variance level of 10^{-7} for ppf (parallel particle filter), this is so that the variance is banlanced with the square bias, which we have already obtained. This is done by using $C=10^6$ on a single processor, with $M=1$.\n",
    "\n",
    "2. Note that the PPF estimator has variance $Var(\\sum_{i=1}^{M}\\Xi_{i}) = \\mathcal{O}(C^{-1}M^{-1})$, this means we can achieve the same variance level by using $C=10^3$ and $M=10^3$. We use $10^3$ parallel cores to obtain $i.i.d.$ realizations of $\\Xi$ at the same time, this will give us a giant speed up. The simulation is set out to find how much is the speed up, at the same time ensuring $Var(\\sum_{i=1}^{M}\\Xi_{i}) \\approx Bias(\\sum_{i=1}^{M}\\Xi_{i}) \\approx 10^{-7}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "T = 100\n",
    "data_path = np.load('ou_model_data_path.npy')\n",
    "c2, c3, beta = np.load('ou_fit_values.npy')\n",
    "max_val=2\n",
    "M=1000\n",
    "const=1000\n",
    "true_val = Kalmanfilter(T,data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Implementaion of PPF\n",
    "1. We need to parallel compute the $M$ realizations. We record the time needed for such one parallel realization.\n",
    "2. We check the MSE of such PPF with $M$ values, this can be done in any fashion.\n",
    "3. We can then compare MLPF with PPF 's cost for similar MSE targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "# Used to construct a parallel - PPF: evaluate the cost of it\n",
    "# Use M cores to get M repe of it and record the time\n",
    "def multi_xi(seed_val):\n",
    "    l_max = max_val\n",
    "    np.random.seed(seed_val)\n",
    "    l = int(np.random.choice(l_max, 1, p=l_prob)[0])\n",
    "    #print('value of l is',l)\n",
    "    if l==0:\n",
    "        Xi = Xi_zero(T,p_prob,p_max,const,Y)\n",
    "    if l!=0:\n",
    "        Xi = Xi_nonzero(l,T,p_prob,p_max,const,Y)\n",
    "    est = Xi / l_prob[l]\n",
    "    return est\n",
    "\n",
    "# Used to obtain MSE of PPF with M. \n",
    "# Use Rep_num of cores to get repetition of it and compute the (sample) MSE.\n",
    "def multi_ppf(seed_val):\n",
    "    np.random.seed(seed_val)\n",
    "    l_max = max_val\n",
    "    p_max = max_val\n",
    "    l_prob = prob_l_func(l_max)\n",
    "    p_prob = prob_p_func(p_max)\n",
    "    est_summand = np.zeros(M)\n",
    "    for m in range(M):\n",
    "        est_summand[m] = Xi(T,l_prob,l_max,p_prob,p_max,const,Y)\n",
    "    return (np.mean(est_summand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MPI4py HPC Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "iter_num = 0\n",
    "\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "## Every iteration should have different initial_seed values\n",
    "initial_seed = iter_num*(size)\n",
    "\n",
    "seed_val_rankwise = initial_seed + rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (I) Cost record of M parallel implementations for PPF estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "stime = time.time()\n",
    "xi_reptition = np.zeros(1)\n",
    "xi_reptition = multi_xi(seed_val_rankwise)\n",
    "\n",
    "result = np.zeros(size)\n",
    "comm.Gather(xi_reptition,result,root=0)\n",
    "if rank == 0 :\n",
    "    x = np.asarray(result)\n",
    "    ppf_estimate = np.mean(x)\n",
    "    print('HPC-PPF outputs:',ppf_estimate)\n",
    "\n",
    "etime = time.time()\n",
    "time_len = str(datetime.timedelta(seconds=etime-stime))\n",
    "print(\"Time cost for HPC-PPF is:\",time_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (II) MSE compuation for PPF estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "ppf_reptition = np.zeros(1)\n",
    "ppf_reptition = multi_ppf(seed_val_rankwise)\n",
    "\n",
    "result = np.zeros(size)\n",
    "comm.Gather(xi_reptition,result,root=0)\n",
    "if rank == 0 :\n",
    "    x = np.asarray(result)\n",
    "    \n",
    "    mse_ppf = np.mean((x-true_val)**2)\n",
    "    var_ppf = np.var(x)\n",
    "    square_bias_ppf = mse_ppf - var_ppf\n",
    "    \n",
    "    print('HPC-PPF has MSE:',mse_ppf, 'Variance:',var_ppf, 'Square Bias:',square_bias_ppf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

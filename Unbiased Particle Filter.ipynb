{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import math\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specification: OU Process\n",
    " 1. $dX_{t} = \\theta_{1}(\\theta_{2} - X_{t})dt + \\sigma dW_{t}$, $Y_{t}|X_{t} \\sim \\mathcal{N}(X_{t}, \\theta_{3}^2)$\n",
    " 2. $\\mathbb{E}[X_{t}] = x_{0} e^{-\\theta_1t} + \\theta_{2} (1-e^{-\\theta_{1}t})$, $Var[X_{t}] = \\frac{\\sigma^{2}}{2\\theta_{1}}(1-e^{-2t\\theta_1})$\n",
    " 3. $Y_{1},Y_{2},...$ mutually independent, $Y_{t} \\sim_{i.i.d.} \\mathcal{N}(\\mathbb{E}[X_{t}], \\theta_{3}^2 + Var[X_{t}])$, for $t \\in \\mathbb{N}_{0}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "initial_val = 1\n",
    "sigma = 0.5\n",
    "theta = np.array([1,0,np.sqrt(0.2)])\n",
    "\n",
    "\n",
    "def diff_coef(x, dt, dw):\n",
    "    return sigma*np.math.sqrt(dt)*dw\n",
    "\n",
    "def drift_coef(x, dt):\n",
    "    return theta[0]*(theta[1]-x)*dt\n",
    "\n",
    "# Log-scaled unnormalized likelihood function p(y|x)\n",
    "def likelihood_logscale(y, x):\n",
    "    d = (y-x)\n",
    "    gn = -1/2*(d**2/(theta[2]**2))\n",
    "    return gn\n",
    "\n",
    "def likelihood_update(y,un,unormal_weight):\n",
    "    gamma = math.sqrt(0.2)\n",
    "    d = (y-un)\n",
    "    gn1 = -1/2*(d**2/(theta[2]**2)) + unormal_weight\n",
    "    return gn1\n",
    "\n",
    "def sig_mean(t,theta):\n",
    "    return initial_val*np.exp(-theta[0]*t) + theta[1]*(1-np.exp(-theta[0]*t))\n",
    "\n",
    "## Used only when theta[0] != 0\n",
    "def sig_var(t,theta):\n",
    "    return (sigma**2 / (2*theta[0])) * (1-np.exp(-2*theta[0]*t))\n",
    "\n",
    "def gen_data(T):\n",
    "    Y = np.zeros(T+1)\n",
    "    for t in range(T+1):\n",
    "        std = np.sqrt(sig_var(t,theta) + theta[2]**2)\n",
    "        Y[t] = sig_mean(t,theta) + std * np.random.randn(1)\n",
    "    return Y\n",
    "\n",
    "def Kalmanfilter(T,Y):\n",
    "    \n",
    "    m = np.zeros((T+1))\n",
    "    mhat = np.zeros((T+1))\n",
    "    c = np.zeros((T+1))\n",
    "    a = theta[0]\n",
    "    s = sigma\n",
    "    # observational noise variance is gam^2*I\n",
    "    gam = theta[2]\n",
    "    # dynamics noise variance is sig^2*I\n",
    "    sig = np.sqrt(s**2/2/a*(1-np.exp(-2*a)))\n",
    "    # dynamics determined by A\n",
    "    A = np.exp(-a)\n",
    "    # initial mean&covariance\n",
    "    m[0] = initial_val\n",
    "    c[0] = 0\n",
    "    H = 1\n",
    "    # solution & assimilate!\n",
    "    for t in range(T):\n",
    "        mhat[t] = A*m[t] + theta[1]*(1-A)\n",
    "        chat = A*c[t]*A + sig**2\n",
    "        ########################\n",
    "        d = Y[t+1] - H*mhat[t]\n",
    "        # Kalmab Gain\n",
    "        K = (chat*H) / (H*chat*H + gam**2)\n",
    "        # Mean Update\n",
    "        m[t+1] = mhat[t] + K*d\n",
    "        # Covariance update\n",
    "        c[t+1] = (1-K*H)*chat\n",
    "    tv = m[T]\n",
    "    return tv\n",
    "\n",
    "def Kalmanfilter_path(T,Y):\n",
    "    \n",
    "    m = np.zeros((T+1))\n",
    "    mhat = np.zeros((T+1))\n",
    "    c = np.zeros((T+1))\n",
    "    a = theta[0]\n",
    "    s = sigma\n",
    "    # observational noise variance is gam^2*I\n",
    "    gam = theta[2]\n",
    "    # dynamics noise variance is sig^2*I\n",
    "    sig = np.sqrt(s**2/2/a*(1-np.exp(-2*a)))\n",
    "    # dynamics determined by A\n",
    "    A = np.exp(-a)\n",
    "    # initial mean&covariance\n",
    "    m[0] = initial_val\n",
    "    c[0] = 0\n",
    "    H = 1\n",
    "    # solution & assimilate!\n",
    "    for t in range(T):\n",
    "        mhat[t] = A*m[t] + theta[1]*(1-A)\n",
    "        chat = A*c[t]*A + sig**2\n",
    "        ########################\n",
    "        d = Y[t+1] - H*mhat[t]\n",
    "        # Kalmab Gain\n",
    "        K = (chat*H) / (H*chat*H + gam**2)\n",
    "        # Mean Update\n",
    "        m[t+1] = mhat[t] + K*d\n",
    "        # Covariance update\n",
    "        c[t+1] = (1-K*H)*chat\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Resampling - input one-dimensional particle x\n",
    "def resampling(weight, gn, x, N):\n",
    "    ess = 1/((weight**2).sum())\n",
    "    if ess <= (N/2):\n",
    "        ## Sample with uniform dice\n",
    "        dice = np.random.random_sample(N)\n",
    "        ## np.cumsum obtains CDF out of PMF\n",
    "        bins = np.cumsum(weight)\n",
    "        ## np.digitize gets the indice of the bins where the dice belongs to \n",
    "        x_hat = x[np.digitize(dice,bins)]\n",
    "        ## after resampling we reset the accumulating weight\n",
    "        gn = np.zeros(N)\n",
    "    if ess > (N/2):\n",
    "        x_hat = x\n",
    "    \n",
    "    return x_hat, gn\n",
    "\n",
    "# Coupled Wasserstein Resampling\n",
    "def coupled_wasserstein(fine_weight, coarse_weight, gn, gc, fine_par, coarse_par, N):\n",
    "    ess = 1/((fine_weight**2).sum())\n",
    "    fine_hat = fine_par\n",
    "    coarse_hat = coarse_par\n",
    "    if ess <= (N/2):\n",
    "        # Sort in ascending order of particles\n",
    "        ind = np.argsort(fine_par[:])\n",
    "        inc = np.argsort(coarse_par[:])\n",
    "        fine_par = fine_par[ind]\n",
    "        fine_weight = fine_weight[ind]\n",
    "        coarse_par = coarse_par[inc]\n",
    "        coarse_weight = coarse_weight[inc]\n",
    "        # Sample with uniform dice\n",
    "        dice = np.random.random_sample(N)\n",
    "        # CDF\n",
    "        bins = np.cumsum(fine_weight)\n",
    "        bins1 = np.cumsum(coarse_weight)\n",
    "        # get the indices of the bins where the dice belongs to\n",
    "        fine_hat = fine_par[np.digitize(dice, bins)]\n",
    "        coarse_hat = coarse_par[np.digitize(dice, bins1)]\n",
    "        # reset accumulating weight after resampling\n",
    "        gn = np.zeros(N)\n",
    "        gc = np.zeros(N)\n",
    "    if ess > (N/2):\n",
    "        fine_hat = fine_par\n",
    "        coarse_hat = coarse_par\n",
    "\n",
    "    return fine_hat, gn, coarse_hat, gc\n",
    "\n",
    "# Maixmally Coupled Resampling\n",
    "def coupled_maximal(fine_weight, coarse_weight, gn, gc, fine_par, coarse_par, N):\n",
    "    ess = 1/((fine_weight**2).sum())\n",
    "    if ess <= (N/2):\n",
    "        # Maximal coupled resampling\n",
    "        fine_hat, coarse_hat = maximal_resample(fine_weight, coarse_weight, fine_par, coarse_par, N)\n",
    "        # reset accumulating weight after resampling\n",
    "        gn = np.zeros(N)\n",
    "        gc = np.zeros(N)\n",
    "    if ess > (N/2):\n",
    "        fine_hat = fine_par\n",
    "        coarse_hat = coarse_par\n",
    "    return fine_hat, gn, coarse_hat, gc\n",
    "\n",
    "def maximal_resample(weight1,weight2,x1,x2,N):\n",
    "    # Initialize\n",
    "    x1_hat = np.zeros(N)\n",
    "    x2_hat = np.zeros(N)\n",
    "\n",
    "    # Calculating many weights\n",
    "    unormal_min_weight = np.minimum(weight1, weight2)\n",
    "    min_weight_sum = np.sum(unormal_min_weight)\n",
    "    min_weight = unormal_min_weight / min_weight_sum\n",
    "    unormal_reduce_weight1 = weight1 - unormal_min_weight\n",
    "    unormal_reduce_weight2 = weight2 - unormal_min_weight\n",
    "\n",
    "    ## Sample with uniform dice\n",
    "    dice = np.random.random_sample(N)\n",
    "    ## [0] takes out the numpy array which is suitable afterwards\n",
    "    coupled = np.where(dice <= min_weight_sum)[0]\n",
    "    independ = np.where(dice > min_weight_sum)[0]\n",
    "    ncoupled = np.sum(dice <= min_weight_sum)\n",
    "    nindepend = np.sum(dice > min_weight_sum)\n",
    "    \n",
    "    if ncoupled>=0:\n",
    "        dice1 = np.random.random_sample(ncoupled)\n",
    "        bins = np.cumsum(min_weight)\n",
    "        x1_hat[coupled] = x1[np.digitize(dice1,bins)]\n",
    "        x2_hat[coupled] = x2[np.digitize(dice1,bins)]\n",
    "   \n",
    "    ## nindepend>0 implies min_weight_sum>0 imples np.sum(unormal_reduce_weight*) is positive, thus the division won't report error\n",
    "    if nindepend>0:\n",
    "        reduce_weight1 = unormal_reduce_weight1 / np.sum(unormal_reduce_weight1)\n",
    "        reduce_weight2 = unormal_reduce_weight2 / np.sum(unormal_reduce_weight2)\n",
    "        dice2 = np.random.random_sample(nindepend)\n",
    "        bins1 = np.cumsum(reduce_weight1)\n",
    "        bins2 = np.cumsum(reduce_weight2)\n",
    "        x1_hat[independ] = x1[np.digitize(dice2,bins1)]\n",
    "        x2_hat[independ] = x2[np.digitize(dice2,bins2)]\n",
    "        \n",
    "    return x1_hat, x2_hat\n",
    "\n",
    "\n",
    "def Particle_filter(l,T,N,Y):\n",
    "    hl = 2**(-l)\n",
    "    un = np.zeros(N)+initial_val\n",
    "    un_hat = un\n",
    "    gn = np.zeros(N)\n",
    "    for t in range(T):\n",
    "        un_hat = un\n",
    "        for dt in range(2**l):\n",
    "            dw = np.random.randn(N)\n",
    "            un = un + drift_coef(un, hl) + diff_coef(un, hl, dw)\n",
    "        # Cumulating weight function    \n",
    "        gn = likelihood_logscale(Y[t+1], un) + gn\n",
    "        what = np.exp(gn-np.max(gn))\n",
    "        wn = what/np.sum(what)\n",
    "        \n",
    "        # Wasserstein resampling\n",
    "        un_hat, gn = resampling(wn, gn, un, N)\n",
    "    \n",
    "    return(np.sum(un*wn))\n",
    "\n",
    "\n",
    "def Coupled_particle_filter_wasserstein(l,T,N,Y):\n",
    "    hl = 2**(-l)\n",
    "    ## Initial value\n",
    "    un1 = np.zeros(N) + initial_val\n",
    "    cn1 = np.zeros(N) + initial_val\n",
    "    gn = np.ones(N)\n",
    "    gc = np.ones(N)\n",
    "    for t in range(T):\n",
    "        un = un1\n",
    "        cn = cn1\n",
    "        for dt in range(2**(l-1)):\n",
    "            dw = np.random.randn(2,N)\n",
    "            for s in range(2):\n",
    "                un = un + drift_coef(un, hl) + diff_coef(un, hl, dw[s,:])\n",
    "            cn = cn + drift_coef(cn, hl*2) + diff_coef(cn, hl, (dw[0,:] + dw[1,:]))\n",
    "        \n",
    "        ## Accumulating Weight Function\n",
    "        gn = likelihood_update(Y[t+1], un, gn)\n",
    "        what = np.exp(gn-np.max(gn))\n",
    "        wn = what/np.sum(what)\n",
    "        gc = likelihood_update(Y[t+1], cn, gc)\n",
    "        wchat = np.exp(gc-np.max(gc))\n",
    "        wc = wchat/np.sum(wchat)\n",
    "        ## Wassersteing Resampling \n",
    "        un1, gn, cn1, gc = coupled_wasserstein(wn,wc,gn,gc,un,cn,N)\n",
    "\n",
    "    return(np.sum(un*wn-cn*wc))\n",
    "\n",
    "\n",
    "def Coupled_particle_filter_maximal(l,T,N,Y):\n",
    "    hl = 2**(-l)\n",
    "    ## Initial value\n",
    "    un1 = np.zeros(N) + initial_val\n",
    "    cn1 = np.zeros(N) + initial_val\n",
    "    gn = np.ones(N)\n",
    "    gc = np.ones(N)\n",
    "    for t in range(T):\n",
    "        un = un1\n",
    "        cn = cn1\n",
    "        for dt in range(2**(l-1)):\n",
    "            dw = np.random.randn(2,N)\n",
    "            for s in range(2):\n",
    "                un = un + drift_coef(un, hl) + diff_coef(un, hl, dw[s,:])\n",
    "            cn = cn + drift_coef(cn, hl*2) + diff_coef(cn, hl, (dw[0,:] + dw[1,:]))\n",
    "        \n",
    "        ## Accumulating Weight Function\n",
    "        gn = likelihood_update(Y[t+1], un, gn)\n",
    "        what = np.exp(gn-np.max(gn))\n",
    "        wn = what/np.sum(what)\n",
    "        gc = likelihood_update(Y[t+1], cn, gc)\n",
    "        wchat = np.exp(gc-np.max(gc))\n",
    "        wc = wchat/np.sum(wchat)\n",
    "        ## Wassersteing Resampling \n",
    "        un1, gn, cn1, gc = coupled_maximal(wn,wc,gn,gc,un,cn,N)\n",
    "\n",
    "    return(np.sum(un*wn-cn*wc))\n",
    "\n",
    "def coef(x, y): \n",
    "    # number of observations/points \n",
    "    n = np.size(x) \n",
    "  \n",
    "    # mean of x and y vector \n",
    "    m_x, m_y = np.mean(x), np.mean(y) \n",
    "  \n",
    "    # calculating cross-deviation and deviation about x \n",
    "    SS_xy = np.sum(y*x) - n*m_y*m_x \n",
    "    SS_xx = np.sum(x*x) - n*m_x*m_x \n",
    "  \n",
    "    # calculating regression coefficients \n",
    "    b_1 = SS_xy / SS_xx \n",
    "    b_0 = m_y - b_1*m_x \n",
    "  \n",
    "    return(b_0, b_1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on one Model and one Dataset, we need to fit:\n",
    "1. $\\mathbb{E}[(\\eta_{t}^{l,N}(\\varphi) - \\eta_{t}^{l}(\\varphi))^2] = C_{2} \\frac{1}{N}$ \n",
    "2. $\\mathbb{E}[\\big((\\eta_{t}^{l}-\\eta_{t}^{l-1})^{N}(\\varphi) - (\\eta_{t}^{l}-\\eta_{t}^{l-1})(\\varphi)\\big)^2] = C_{3} \\frac{\\Delta_{l}^{\\beta}}{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to tune values of C_2\n",
    "def fit_c2(data_path):\n",
    "    rep_num = 100\n",
    "    num_seq = np.zeros(6)\n",
    "    var_pf = np.zeros(6)\n",
    "    T = data_path.shape[0]-1\n",
    "    for i in range(6):\n",
    "        num_seq[i] = 100 * 2**i\n",
    "        rep_val = np.zeros(rep_num)\n",
    "        #pr = progressbar.ProgressBar(max_value=rep_num).start()\n",
    "        for j in range(rep_num):\n",
    "            rep_val[j] = Particle_filter(0,T,int(num_seq[i]),data_path)\n",
    "            #pr.update(j+1)\n",
    "        #pr.finish()\n",
    "        print(i,'in 6 finished')\n",
    "        var_pf[i] = np.var(rep_val)\n",
    "        \n",
    "    x = np.log10(num_seq)\n",
    "    y = np.log10(var_pf)\n",
    "    b=coef(x,y)\n",
    "    print('slope is:',b[1])\n",
    "    print('c2 value:',10**(b[0]))\n",
    "    return 10**(b[0])\n",
    "\n",
    "# Function to tune values of C_3, as well as values of beta\n",
    "def fit_c3_beta(data_path):\n",
    "    rep_num = 100\n",
    "    N = 200\n",
    "    l_seq = np.zeros(6)\n",
    "    delt_seq = np.zeros(6)\n",
    "    var_cpf = np.zeros(6)\n",
    "    T = data_path.shape[0]-1\n",
    "    for i in range(6):\n",
    "        l_seq[i] = i+1\n",
    "        delt_seq[i] = 2**(-(i+1))\n",
    "        rep_val = np.zeros(rep_num)\n",
    "        #pr = progressbar.ProgressBar(max_value=rep_num).start()\n",
    "        for j in range(rep_num):\n",
    "            rep_val[j] = Coupled_particle_filter_maximal(int(l_seq[i]),T,N,data_path)\n",
    "            #pr.update(j+1)\n",
    "        #pr.finish()\n",
    "        print(i,'in 6 finished')        \n",
    "        var_cpf[i] = np.var(rep_val)\n",
    "        \n",
    "    x = np.log10(delt_seq)\n",
    "    y = np.log10(var_cpf)\n",
    "    b=coef(x,y)\n",
    "    beta = b[1]\n",
    "    print('beta decimal value is:',b[1])\n",
    "    print('c3 value:',10**(b[0])*N)\n",
    "    return 10**(b[0])*N, round(b[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Particle Filter: Untuned\n",
    "\n",
    "1. Choice of truncated distribution $\\mathbb{P}_{P}(p) = 2^{-p}$ for $p \\in \\{0,1,...,P_{max}\\}$, $\\mathbb{P}_{L}(l) = 2^{-\\beta l}$ for $l \\in \\{1,2,...,L_{max}\\}$, $L_{max} = P_{max}$.\n",
    "2. $N_{p} = 2^{p}N_{0}$, $N_{0} = C P_{max}^{2}2^{2P_{max}}$ $\\Delta_{l}=2^{-l\\beta}$\n",
    "3. For MSE target of $\\mathcal{O}(\\epsilon^{2})$, we need cost of $\\mathcal{O}(\\epsilon^{-2} (\\log(\\epsilon))^{3})$ when $\\beta=2$, $\\mathcal{O}(\\epsilon^{-2} (\\log(\\epsilon))^{4})$ when $\\beta=1$ and $\\mathcal{O}(\\epsilon^{-2.5} (\\log(\\epsilon))^{3})$ when $\\beta=\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Particle Filter:\n",
    "1. if $l=0$, $N_{p}=C C_{2} P_{max}^{2}2^{2P_{max}}$ \n",
    "2. if $l>0$, $N_{p}=C C_{3} P_{max}^{2}2^{2P_{max}}$\n",
    "3. The constant $C$ is tuned so that the MSE of the PPF estimator is of the same order (roughly twice) as its variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def num_coupled_par(p, p_max, const):\n",
    "    return int(2**(p+2*p_max) * (p_max**2) * const * c3)\n",
    "\n",
    "def num_par(p, p_max, const):\n",
    "    return int(2**(p+2*p_max) * (p_max**2) * const * c2)\n",
    "\n",
    "def prob_l_func(max_val):\n",
    "    prob = np.zeros(max_val)\n",
    "    for l in range(max_val):\n",
    "        prob[l] = 2**(-l*beta)\n",
    "    prob = prob / np.sum(prob)\n",
    "    return prob\n",
    "\n",
    "def prob_p_func(max_val):\n",
    "    prob = np.zeros(max_val)\n",
    "    for p in range(max_val):\n",
    "        prob[p] = 2**(-p)\n",
    "    prob = prob / np.sum(prob)\n",
    "    return prob\n",
    "\n",
    "def Xi_zero(T,p_prob,p_max,const,Y):\n",
    "    # sample the variable P\n",
    "    p = int(np.random.choice(p_max, 1, p=p_prob)[0])\n",
    "    #print('p_val is',p)\n",
    "    # construct the estimator\n",
    "    Xi_zero = (Particle_filter(0,T,num_par(p, p_max, const),Y) - Particle_filter(0,T,num_par(p-1, p_max, const),Y)) / p_prob[p]\n",
    "    return Xi_zero\n",
    "\n",
    "def Xi_nonzero(l,T,p_prob,p_max,const,Y):\n",
    "    # sample the variable P\n",
    "    p = int(np.random.choice(p_max, 1, p=p_prob)[0])\n",
    "    #print('p_val is',p)\n",
    "    # construct the estimator\n",
    "    Xi = (Coupled_particle_filter_maximal(l,T,num_coupled_par(p,p_max,const),Y) - Coupled_particle_filter_maximal(l,T,num_coupled_par(p-1,p_max,const),Y)) / p_prob[p]\n",
    "    return Xi\n",
    "\n",
    "def Xi(T,l_prob,l_max,p_prob,p_max,const,Y):\n",
    "    l = int(np.random.choice(l_max, 1, p=l_prob)[0])\n",
    "    #print('value of l is',l)\n",
    "    if l==0:\n",
    "        Xi = Xi_zero(T,p_prob,p_max,const,Y)\n",
    "    if l!=0:\n",
    "        Xi = Xi_nonzero(l,T,p_prob,p_max,const,Y)\n",
    "    est = Xi / l_prob[l]\n",
    "    return est\n",
    "    \n",
    "def parallel_particle_filter(M,T,max_val,const,Y):\n",
    "    l_max = max_val\n",
    "    p_max = max_val\n",
    "    l_prob = prob_l_func(l_max)\n",
    "    p_prob = prob_p_func(p_max)\n",
    "    est_summand = np.zeros(M)\n",
    "    for m in range(M):\n",
    "        est_summand[m] = Xi(T,l_prob,l_max,p_prob,p_max,const,Y)\n",
    "    return (np.mean(est_summand))\n",
    "\n",
    "def parallel_particle_filter_record(M,T,max_val,const,Y):\n",
    "    l_max = max_val\n",
    "    p_max = max_val\n",
    "    l_prob = prob_l_func(l_max)\n",
    "    p_prob = prob_p_func(p_max)\n",
    "    est_summand = np.zeros(M)\n",
    "    pr = progressbar.ProgressBar(max_value=M).start()\n",
    "    for m in range(M):\n",
    "        est_summand[m] = Xi(T,l_prob,l_max,p_prob,p_max,const,Y)\n",
    "        pr.update(m+1)\n",
    "    pr.finish()\n",
    "    return est_summand\n",
    "\n",
    "def Xi_zero_with_p(T,p_prob,p_max,const,Y):\n",
    "    # sample the variable P\n",
    "    p = int(np.random.choice(p_max, 1, p=p_prob)[0])\n",
    "    #print('p_val is',p)\n",
    "    # construct the estimator\n",
    "    Xi_zero = (Particle_filter(0,T,num_par(p, p_max, const),Y) - Particle_filter(0,T,num_par(p-1, p_max, const),Y)) / p_prob[p]\n",
    "    return Xi_zero, p\n",
    "\n",
    "def Xi_nonzero_with_p(l,T,p_prob,p_max,const,Y):\n",
    "    # sample the variable P\n",
    "    p = int(np.random.choice(p_max, 1, p=p_prob)[0])\n",
    "    #print('p_val is',p)\n",
    "    # construct the estimator\n",
    "    Xi = (Coupled_particle_filter_maximal(l,T,num_coupled_par(p,p_max,const),Y) - Coupled_particle_filter_maximal(l,T,num_coupled_par(p-1,p_max,const),Y)) / p_prob[p]\n",
    "    return Xi, p\n",
    "\n",
    "def Xi_with_pl(T,l_prob,l_max,p_prob,p_max,const,Y):\n",
    "    l = int(np.random.choice(l_max, 1, p=l_prob)[0])\n",
    "    #print('value of l is',l)\n",
    "    if l==0:\n",
    "        Xi, p_val = Xi_zero_with_p(T,p_prob,p_max,const,Y)\n",
    "    if l!=0:\n",
    "        Xi, p_val = Xi_nonzero_with_p(l,T,p_prob,p_max,const,Y)\n",
    "    est = Xi / l_prob[l]\n",
    "    return est, l, p_val\n",
    "\n",
    "def cost_proxy_ppf(p_collect,l_collect, max_val, const):\n",
    "    M = p_collect.shape[0]\n",
    "    cost_proxy_val = 0\n",
    "    for i in range(M):\n",
    "        if l_collect[i] == 0:\n",
    "            cost_proxy_val += num_par(p_collect[i], max_val, const)\n",
    "        if l_collect[i] != 0:\n",
    "            cost_proxy_val += num_coupled_par(p_collect[i], max_val, const) * 2**(l_collect[i])\n",
    "    return cost_proxy_val\n",
    "\n",
    "def parallel_particle_filter_record_with_cost(M,T,max_val,const,Y):\n",
    "    l_max = max_val\n",
    "    p_max = max_val\n",
    "    l_prob = prob_l_func(l_max)\n",
    "    p_prob = prob_p_func(p_max)\n",
    "    est_summand = np.zeros(M)\n",
    "    p_collect = np.zeros(M)\n",
    "    l_collect = np.zeros(M)\n",
    "    pr = progressbar.ProgressBar(max_value=M).start()\n",
    "    for m in range(M):\n",
    "        est_summand[m], p_collect[m], l_collect[m] = Xi_with_pl(T,l_prob,l_max,p_prob,p_max,const,Y)\n",
    "        pr.update(m+1)\n",
    "    pr.finish()\n",
    "    \n",
    "    cost_proxy_val = T * cost_proxy_ppf(p_collect,l_collect, max_val, const)\n",
    "    \n",
    "    return est_summand, cost_proxy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilevel Particle Filter: Untuned\n",
    "1. When $\\beta=2$, $N_{l} = 2^{2L-1.5l}$, to target a MSE of $\\epsilon^{2}$, cost required is $\\mathcal{O}(\\epsilon^{-2})$\n",
    "2. When $\\beta=1$, $N_{l} = 2^{2L-l}L$, to target a MSE of $\\epsilon^{2}$, cost required is $\\mathcal{O}(\\epsilon^{-2}(\\log(\\epsilon))^{2})$\n",
    "3. When $\\beta=\\frac{1}{2}$, $N_{l} = 2^{2.25L - 0.75l}$, to target a MSE of $\\epsilon^{2}$, cost required is $\\mathcal{O}(\\epsilon^{-2.5})$\n",
    "\n",
    "\n",
    "#### Multilevel Particle Filter: \n",
    "1. When $\\beta=2$, $N_{0} = C_{1}C_{2}2^{2L}$, $N_{l}=C_{1}C_{3}2^{2L-1.5l}$\n",
    "2. When $\\beta=1$, $N_{0}=C_{1}C_{2}2^{2L}L$, $N_{l}=C_{1}C_{3}2^{2L-l}L$\n",
    "3. When $\\beta=\\frac{1}{2}$, $N_{0}=C_{1}C_{2}2^{2.25L}$, $N_{l}=C_{1}C_{3}2^{2.25L-0.75l}$\n",
    "4. The constant $C_{1}$ is choosen such that the MSE of MLPF estimator $\\eta_{t}^{L,N_{0:L}}$ is of same order (roughly twice) as its variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For OU process, beta=2\n",
    "def num_ml_coupled(l,lmax,const):\n",
    "    return 2**(2*lmax-1.5*l) * const * c3\n",
    "\n",
    "def num_ml_single(l,lmax,const):\n",
    "    return 2**(2*lmax-1.5*l) * const * c2\n",
    "\n",
    "def mlpf(T,max_val,const,Y):\n",
    "    L = max_val\n",
    "    level_est = np.zeros(L)\n",
    "    level_est[0] = Particle_filter(0,T,int(num_ml_single(0,L,const)),Y)\n",
    "    for l in range(1,L):\n",
    "        level_est[l] = Coupled_particle_filter_maximal(l,T,int(num_ml_coupled(l,L,const)),Y)\n",
    "    return np.sum(level_est)\n",
    "\n",
    "def proxy_cost_mlpf(T,max_val,const):\n",
    "    cost_proxy_val = 0\n",
    "    cost_proxy_val += T*num_ml_single(0,max_val,const)\n",
    "    for l in range(max_val):\n",
    "        cost_proxy_val += T*num_ml_coupled(l,max_val,const) * 2**(l)\n",
    "    return cost_proxy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data set for specific OU model used in HPC implementaion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "data_path = np.load('ou_model_data_path.npy')\n",
    "c2, c3, beta = np.load('ou_fit_values.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Example: Comparing MLPF with PPF on sigle processor\n",
    "1. For a given $L_{max}$ values, for instance $L_{max}=2$, we first tune the constant $C$ for the PPF (parallel particle filter). We denote the PPF estimator as $\\frac{1}{M}\\sum_{i=1}^{M}\\Xi^{i}$, on single processor, we assume $M=1$, we check the value of $Var(\\Xi)=\\mathcal{O}(C^{-1})$ for any initial guess on $C$ value, and then obtain the true $C$ by ensuring the variance of the PPF estimator is roughly equal to its squared bias. In this case, we should set $C=1000000$.\n",
    "2. Computing Time of PPF estimator, this can be extremly costly to run, proxy cost represented by the number of Euler discretizations is used.\n",
    "3. We include also cell to count the actual computing time for PPF, as for the HPC implementation, we will compare instead the actual computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:03:35 Time:  0:03:35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At level 2 Mse val: 0.0003864107054792948 Var val: 0.00038618142532472404 Square Bias val: 2.2928015457073956e-07\n"
     ]
    }
   ],
   "source": [
    "# Rep_num here is different from M, we record all the xi values and take variance\n",
    "const = 1000\n",
    "true_val = Kalmanfilter(T,data_path)\n",
    "mse_seq = np.zeros(6)\n",
    "var_seq = np.zeros(6)\n",
    "square_bias_seq = np.zeros(6)\n",
    "l_seq = np.arange(2,3)\n",
    "rep_num = 1000\n",
    "for i,lmax in enumerate(l_seq):\n",
    "    est_val = parallel_particle_filter_record(rep_num, T, lmax, const, data_path)\n",
    "    mse_seq[i] = np.mean((est_val-true_val)**2)\n",
    "    var_seq[i] = np.var(est_val)\n",
    "    square_bias_seq[i] = mse_seq[i] - var_seq[i] \n",
    "    print('At level',lmax,'Mse val:',mse_seq[i], 'Var val:', var_seq[i], 'Square Bias val:', square_bias_seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Time for a single replication\n",
    "lmax = 1\n",
    "const = 1000000\n",
    "%timeit parallel_particle_filter(1, T, lmax, const, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy Time Cost for a single replication\n",
    "lmax = 1\n",
    "const = 1000000\n",
    "cost_proxy_val = parallel_particle_filter_record_with_cost(1, T, lmax, const, data_path)[1]\n",
    "print('Estimated Cost for PPF estimator:', cost_proxy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In order to achieve similar MSE levels, we test and see that $L=6$ is required for MLPF estimator. In order to keep its variance to be roughly the same as its squared bias, we tune $C_{1}$ in a similar way. We can conlude that $C_{1}=600$ here.\n",
    "5. Again the cost is evaluated through a proxy, the number of Euler discretizations involved to construct the MLPF estimator.\n",
    "6. We include also cell to count the actual computing time for MLPF, as for the HPC implementation, we will compare instead the actual computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:03:50 Time:  0:03:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At level 6 Mse val: 6.483557920983263e-05 Var val: 6.382529355465471e-05 Square Bias val: 1.0102856551779163e-06\n"
     ]
    }
   ],
   "source": [
    "const = 10\n",
    "true_val = Kalmanfilter(T,data_path)\n",
    "mse_seq = np.zeros(6)\n",
    "var_seq = np.zeros(6)\n",
    "square_bias_seq = np.zeros(6)\n",
    "l_seq = np.arange(6,7)\n",
    "rep_num = 1000\n",
    "for i,lmax in enumerate(l_seq):    \n",
    "    # repe of mlpf estimator\n",
    "    est_val = np.zeros(rep_num)\n",
    "    pr = progressbar.ProgressBar(max_value=rep_num).start()\n",
    "    for j in range(rep_num):\n",
    "        est_val[j] = mlpf(T,lmax,const,data_path)  \n",
    "        pr.update(j+1)\n",
    "    pr.finish()\n",
    "    mse_seq[i] = np.mean((est_val-true_val)**2)\n",
    "    var_seq[i] = np.var(est_val)\n",
    "    square_bias_seq[i] = mse_seq[i] - var_seq[i] \n",
    "    print('At level',lmax,'Mse val:',mse_seq[i], 'Var val:', var_seq[i], 'Square Bias val:', square_bias_seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Computing Time\n",
    "const = 600\n",
    "lmax = 6\n",
    "%timeit mlpf(T,lmax,const,data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proxy Computation Time\n",
    "const = 100\n",
    "lmax = 6\n",
    "proxy_cost_mlpf(T,lmax,const)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
